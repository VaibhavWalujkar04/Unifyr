{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from transformers import BertTokenizer, BertModel\n",
    "from sklearn.decomposition import LatentDirichletAllocation\n",
    "from scipy.optimize import linear_sum_assignment\n",
    "import pandas as pd\n",
    "import faiss\n",
    "import torch\n",
    "from deap import base, creator, tools, algorithms\n",
    "import random\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "\n",
    "# Ignore warnings for cleaner output\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "experience_weight = 0.2 \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample data\n",
    "candidates_data = {\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David'],\n",
    "    'skills': [\n",
    "        'Python, Machine Learning, Data Analysis, SQL, TensorFlow, Pandas, NumPy, Keras, Scikit-Learn, ETL',\n",
    "        'Java, Spring, Microservices, Docker, Kubernetes, Maven, RESTful APIs, Hibernate, Git, CI/CD',\n",
    "        'Python, Deep Learning, NLP, PyTorch, Data Visualization, Flask, AWS, Spark, Big Data, SQL',\n",
    "        'JavaScript, React, Node.js, TypeScript, Web Development, Angular, HTML, CSS, Redux, Webpack'\n",
    "    ],\n",
    "    'experience': [7, 5, 4, 6]  # Years of experience\n",
    "}\n",
    "\n",
    "\n",
    "experts_data = {\n",
    "    'id': [1, 2, 3, 4],\n",
    "    'name': ['Dr. Johnson','Dr. Smith', 'Dr. Brown', 'Dr. Taylor'],\n",
    "    'skills': [\n",
    "        'Python, Machine Learning, Data Analysis, SQL, TensorFlow, Pandas, NumPy, Keras, Scikit-Learn, ETL',\n",
    "        'Java, Spring, Microservices, Docker, Kubernetes, Maven, RESTful APIs, Keras, Scikit-Learn, ETL',\n",
    "        'C++, High-Performance Computing, Algorithm Optimization, Data Structures, Machine Learning, SQL, Parallel Computing, MPI, OpenMP',\n",
    "        'JavaScript, Full Stack Development, React, Node.js, TypeScript, Angular, Web Development, HTML, CSS, Redux'\n",
    "    ],\n",
    "    'experience': [15, 10, 12, 8]  # Years of experience\n",
    "}\n",
    "# Candidate David is best matched with Expert Dr. Taylor (Combined Score: 0.98)\n",
    "# Candidate Alice is best matched with Expert Dr. Johnson (Combined Score: 0.90)\n",
    "# Candidate Bob is best matched with Expert Dr. Smith (Combined Score: 0.76)\n",
    "# Candidate Charlie is best matched with Expert Dr. Johnson (Combined Score: 0.30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_with_BERT_embeddings(candidates_data, experts_data):\n",
    "    # Create DataFrames\n",
    "    candidates_df = pd.DataFrame(candidates_data)\n",
    "    experts_df = pd.DataFrame(experts_data)\n",
    "\n",
    "    # Load BERT model and tokenizer\n",
    "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "    model = BertModel.from_pretrained('bert-base-uncased')\n",
    "\n",
    "    # Function to get BERT embeddings\n",
    "    def get_bert_embedding(text):\n",
    "        inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        # Mean pooling over the token embeddings\n",
    "        embeddings = outputs.last_hidden_state.mean(dim=1)\n",
    "        return embeddings.numpy().flatten()\n",
    "\n",
    "    # Generate embeddings for candidates\n",
    "    candidates_df['embedding'] = candidates_df['skills'].apply(get_bert_embedding)\n",
    "\n",
    "    # Generate embeddings for experts\n",
    "    experts_df['embedding'] = experts_df['skills'].apply(get_bert_embedding)\n",
    "\n",
    "    # Convert the embeddings column into a list of arrays for similarity calculation\n",
    "    candidates_embeddings = np.array(candidates_df['embedding'].tolist())\n",
    "    experts_embeddings = np.array(experts_df['embedding'].tolist())\n",
    "\n",
    "    # Normalize experience values to be between 0 and 1\n",
    "    candidates_df['normalized_experience'] = candidates_df['experience'] / candidates_df['experience'].max()\n",
    "    experts_df['normalized_experience'] = experts_df['experience'] / experts_df['experience'].max()\n",
    "\n",
    "    # Compute cosine similarity based on embeddings\n",
    "    similarity_matrix = cosine_similarity(candidates_embeddings, experts_embeddings)\n",
    "\n",
    "    # Factor in the experience by multiplying the cosine similarity with experience ratios\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        for j in range(similarity_matrix.shape[1]):\n",
    "            # Calculate the absolute difference in experience\n",
    "            experience_diff = abs(candidates_df['experience'].iloc[i] - experts_df['experience'].iloc[j])\n",
    "            \n",
    "            # Compute inverse experience ratio (add a small epsilon to avoid division by zero)\n",
    "            epsilon = 1e-6\n",
    "            experience_ratio = 1 / (experience_diff + epsilon)\n",
    "            \n",
    "            # Normalize the experience ratio (optional, for better scaling)\n",
    "            experience_ratio /= (1 + experience_ratio)\n",
    "            \n",
    "            # Adjust the similarity score\n",
    "            similarity_matrix[i, j] *= (1 - experience_weight + experience_weight * experience_ratio)\n",
    "\n",
    "\n",
    "    # Create a DataFrame to easily visualize the similarities\n",
    "    similarity_df = pd.DataFrame(similarity_matrix, \n",
    "                                 index=candidates_df['name'], \n",
    "                                 columns=experts_df['name'])\n",
    "\n",
    "    # Convert the DataFrame into the desired dictionary format\n",
    "    similarity_dict = {}\n",
    "    for candidate in similarity_df.index:\n",
    "        similarity_dict[candidate] = {}\n",
    "        for expert in similarity_df.columns:\n",
    "            similarity_dict[candidate][expert] = similarity_df.loc[candidate, expert]\n",
    "\n",
    "    return similarity_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Sequential.forward() got an unexpected keyword argument 'input_ids'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[191], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mcosine_similarity_with_BERT_embeddings\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcandidates_data\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexperts_data\u001b[49m\u001b[43m)\u001b[49m)\n",
      "Cell \u001b[0;32mIn[190], line 15\u001b[0m, in \u001b[0;36mcosine_similarity_with_BERT_embeddings\u001b[0;34m(candidates_data, experts_data)\u001b[0m\n\u001b[1;32m     12\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m embeddings\u001b[38;5;241m.\u001b[39mnumpy()\u001b[38;5;241m.\u001b[39mflatten()\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Generate embeddings for candidates\u001b[39;00m\n\u001b[0;32m---> 15\u001b[0m candidates_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mcandidates_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mskills\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mget_bert_embedding\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Generate embeddings for experts\u001b[39;00m\n\u001b[1;32m     18\u001b[0m experts_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124membedding\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m experts_df[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mskills\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(get_bert_embedding)\n",
      "File \u001b[0;32m/var/data/python/lib/python3.11/site-packages/pandas/core/series.py:4630\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4520\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[1;32m   4521\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m   4522\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4525\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m   4526\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[1;32m   4527\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4528\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4529\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4628\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4629\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4630\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/data/python/lib/python3.11/site-packages/pandas/core/apply.py:1025\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1022\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_str()\n\u001b[1;32m   1024\u001b[0m \u001b[38;5;66;03m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1025\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/var/data/python/lib/python3.11/site-packages/pandas/core/apply.py:1076\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1074\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1075\u001b[0m         values \u001b[38;5;241m=\u001b[39m obj\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m)\u001b[38;5;241m.\u001b[39m_values\n\u001b[0;32m-> 1076\u001b[0m         mapped \u001b[38;5;241m=\u001b[39m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m            \u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m            \u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m            \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1082\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1083\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1084\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1085\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[0;32m/var/data/python/lib/python3.11/site-packages/pandas/_libs/lib.pyx:2834\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n",
      "Cell \u001b[0;32mIn[190], line 9\u001b[0m, in \u001b[0;36mcosine_similarity_with_BERT_embeddings.<locals>.get_bert_embedding\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      7\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(text, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m'\u001b[39m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m----> 9\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Mean pooling over the token embeddings\u001b[39;00m\n\u001b[1;32m     11\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m outputs\u001b[38;5;241m.\u001b[39mlast_hidden_state\u001b[38;5;241m.\u001b[39mmean(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m/var/data/python/lib/python3.11/site-packages/torch/nn/modules/module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "\u001b[0;31mTypeError\u001b[0m: Sequential.forward() got an unexpected keyword argument 'input_ids'"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity_with_BERT_embeddings(candidates_data, experts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity_with_topic_modelling(candidates_data, experts_data):\n",
    "    # Convert data to DataFrames\n",
    "    candidates_df = pd.DataFrame(candidates_data)\n",
    "    experts_df = pd.DataFrame(experts_data)\n",
    "\n",
    "    # Combine skills from both candidates and experts\n",
    "    all_skills = pd.concat([candidates_df['skills'], experts_df['skills']])\n",
    "\n",
    "    # Preprocessing - Vectorization using TF-IDF\n",
    "    tfidf_vectorizer = TfidfVectorizer(stop_words='english')\n",
    "    tfidf_matrix = tfidf_vectorizer.fit_transform(all_skills)\n",
    "\n",
    "    # Apply LDA to extract topics\n",
    "    n_topics = 5  # Number of topics to extract\n",
    "    lda_model = LatentDirichletAllocation(n_components=n_topics, random_state=42)\n",
    "    lda_matrix = lda_model.fit_transform(tfidf_matrix)\n",
    "\n",
    "    # Split LDA matrix back into candidates and experts\n",
    "    candidates_lda = lda_matrix[:len(candidates_df)]\n",
    "    experts_lda = lda_matrix[len(candidates_df):]\n",
    "\n",
    "    # Compute cosine similarity between candidates and experts based on topic distribution\n",
    "    similarity_matrix = cosine_similarity(candidates_lda, experts_lda)\n",
    "\n",
    "    # Normalize expert experience and factor it into the similarity score\n",
    "    experts_df['normalized_experience'] = experts_df['experience'] / experts_df['experience'].max()\n",
    "\n",
    "    for i in range(similarity_matrix.shape[0]):\n",
    "        for j in range(similarity_matrix.shape[1]):\n",
    "            # Calculate the absolute difference in experience between candidate and expert\n",
    "            experience_diff = abs(candidates_df['experience'].iloc[i] - experts_df['experience'].iloc[j])\n",
    "            \n",
    "            # Compute inverse experience ratio (add a small epsilon to avoid division by zero)\n",
    "            epsilon = 1e-6\n",
    "            experience_ratio = 1 / (experience_diff + epsilon)\n",
    "            \n",
    "            # Normalize the experience ratio (optional for scaling)\n",
    "            experience_ratio /= (1 + experience_ratio)\n",
    "            \n",
    "            # Adjust similarity score with experience ratio\n",
    "            similarity_matrix[i, j] *= (1 - experience_weight + experience_weight * experience_ratio)\n",
    "\n",
    "    # Convert similarity matrix to nested dictionary format\n",
    "    similarity_dict = {}\n",
    "    for i, candidate in enumerate(candidates_df['name']):\n",
    "        similarity_dict[candidate] = {}\n",
    "        for j, expert in enumerate(experts_df['name']):\n",
    "            similarity_dict[candidate][expert] = similarity_matrix[i][j]\n",
    "\n",
    "    # Display the similarity dictionary\n",
    "    # for candidate, experts in similarity_dict.items():\n",
    "    #     print(f\"\\nSimilarity scores for {candidate}:\")\n",
    "    #     for expert, score in experts.items():\n",
    "    #         print(f\"  {expert}: {score:.6f}\")\n",
    "\n",
    "    return similarity_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alice': {'Dr. Johnson': 0.8333333277777788, 'Dr. Smith': 0.8181818165289259, 'Dr. Brown': 0.0975932183778799, 'Dr. Taylor': 0.0991069981263536}, 'Bob': {'Dr. Johnson': 0.8499945169006395, 'Dr. Smith': 0.8222169279314182, 'Dr. Brown': 0.10026787419950227, 'Dr. Taylor': 0.10680707376988563}, 'Charlie': {'Dr. Johnson': 0.09753588352152301, 'Dr. Smith': 0.0961345061947944, 'Dr. Brown': 0.09922590928830119, 'Dr. Taylor': 0.10085605654726858}, 'David': {'Dr. Johnson': 0.0964960347564007, 'Dr. Smith': 0.09419851078671959, 'Dr. Brown': 0.0983844719840481, 'Dr. Taylor': 0.8666626523483435}}\n"
     ]
    }
   ],
   "source": [
    "print(cosine_similarity_with_topic_modelling(candidates_data, experts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Faiss_search(candidates_data, experts_data):\n",
    "\n",
    "    # Convert data to DataFrames\n",
    "    candidates_df = pd.DataFrame(candidates_data)\n",
    "    experts_df = pd.DataFrame(experts_data)\n",
    "\n",
    "    # Load pre-trained Sentence-BERT model to generate embeddings\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Generate embeddings for candidate and expert skills\n",
    "    candidates_embeddings = model.encode(candidates_df['skills'].tolist())\n",
    "    experts_embeddings = model.encode(experts_df['skills'].tolist())\n",
    "\n",
    "    # Normalize embeddings for better FAISS performance\n",
    "    candidates_embeddings = np.array(candidates_embeddings).astype('float32')\n",
    "    experts_embeddings = np.array(experts_embeddings).astype('float32')\n",
    "\n",
    "    # Create a FAISS index\n",
    "    index = faiss.IndexFlatL2(candidates_embeddings.shape[1])  # L2 distance\n",
    "    index.add(experts_embeddings)  # Add expert embeddings to the index\n",
    "\n",
    "    # Search for the nearest experts for each candidate\n",
    "    k = len(experts_df)  # Search for all experts to get similarity with each expert\n",
    "    distances, indices = index.search(candidates_embeddings, k)\n",
    "\n",
    "    # Normalize expert experience and factor it into the similarity score\n",
    "    experts_df['normalized_experience'] = experts_df['experience'] / experts_df['experience'].max()\n",
    "\n",
    "    # Store the matches and similarity scores for each candidate-expert pair\n",
    "    similarity_scores = {}\n",
    "\n",
    "    # Iterate over candidates and store all similarity scores\n",
    "    for i, candidate in enumerate(candidates_df['name']):\n",
    "        candidate_scores = {}\n",
    "        for j in range(k):\n",
    "            expert_idx = indices[i][j]\n",
    "            expert_name = experts_df.iloc[expert_idx]['name']\n",
    "            similarity_score = 1 / (1 + distances[i][j])  # Convert distance to similarity score\n",
    "            \n",
    "            # Calculate the absolute difference in experience between candidate and expert\n",
    "            experience_diff = abs(candidates_df['experience'].iloc[i] - experts_df['experience'].iloc[expert_idx])\n",
    "            \n",
    "            # Compute inverse experience ratio (add a small epsilon to avoid division by zero)\n",
    "            epsilon = 1e-6\n",
    "            experience_ratio = 1 / (experience_diff + epsilon)\n",
    "            \n",
    "            # Normalize the experience ratio (optional for scaling)\n",
    "            experience_ratio /= (1 + experience_ratio)\n",
    "            \n",
    "            # Adjust the similarity score based on the experience ratio\n",
    "            adjusted_similarity_score = similarity_score * (1 - experience_weight + experience_weight * experience_ratio)\n",
    "            candidate_scores[expert_name] = adjusted_similarity_score\n",
    "        \n",
    "        similarity_scores[candidate] = candidate_scores\n",
    "\n",
    "\n",
    "    # Return similarity scores\n",
    "    return similarity_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alice': {'Dr. Johnson': 0.8333333277777788, 'Dr. Smith': 0.8181818165289259, 'Dr. Brown': 0.4471360019595779, 'Dr. Taylor': 0.3821540124358233}, 'Bob': {'Dr. Taylor': 0.4168642990282711, 'Dr. Johnson': 0.34337471281050685, 'Dr. Smith': 0.33215332085417537, 'Dr. Brown': 0.3288761976265938}, 'Charlie': {'Dr. Johnson': 0.5834837663294539, 'Dr. Smith': 0.5751003807371362, 'Dr. Brown': 0.4115934017350203, 'Dr. Taylor': 0.37401277770004954}, 'David': {'Dr. Taylor': 0.7838279382034288, 'Dr. Johnson': 0.3829458043553021, 'Dr. Smith': 0.3738280497572381, 'Dr. Brown': 0.3438182451372938}}\n"
     ]
    }
   ],
   "source": [
    "print(Faiss_search(candidates_data, experts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "def Bipartite_graph_matching(candidates_data, experts_data):\n",
    "    # Convert data to DataFrames\n",
    "    candidates_df = pd.DataFrame(candidates_data)\n",
    "    experts_df = pd.DataFrame(experts_data)\n",
    "\n",
    "    # Load pre-trained Sentence-BERT model to generate embeddings\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    # Generate embeddings for candidate and expert skills\n",
    "    candidates_embeddings = model.encode(candidates_df['skills'].tolist())\n",
    "    experts_embeddings = model.encode(experts_df['skills'].tolist())\n",
    "\n",
    "    # Compute cosine similarity between each candidate and expert\n",
    "    similarity_matrix = cosine_similarity(candidates_embeddings, experts_embeddings)\n",
    "\n",
    "    # Normalize expert experience and factor it into the similarity scores\n",
    "    experts_df['normalized_experience'] = experts_df['experience'] / experts_df['experience'].max()\n",
    "\n",
    "    # Adjust similarity scores based on expert experience\n",
    "    for candidate_idx in range(len(candidates_df)):\n",
    "        for expert_idx in range(len(experts_df)):\n",
    "            # Calculate the absolute difference in experience between candidate and expert\n",
    "            experience_diff = abs(candidates_df['experience'].iloc[candidate_idx] - experts_df['experience'].iloc[expert_idx])\n",
    "            \n",
    "            # Compute inverse experience ratio (add a small epsilon to avoid division by zero)\n",
    "            epsilon = 1e-6\n",
    "            experience_ratio = 1 / (experience_diff + epsilon)\n",
    "            \n",
    "            # Normalize the experience ratio (optional for scaling)\n",
    "            experience_ratio /= (1 + experience_ratio)\n",
    "            \n",
    "            # Adjust the similarity score based on the experience ratio\n",
    "            similarity_matrix[candidate_idx, expert_idx] *= (\n",
    "                1 - experience_weight + experience_weight * experience_ratio\n",
    "            )\n",
    "\n",
    "\n",
    "    # Create a DataFrame for storing the results\n",
    "    results = []\n",
    "\n",
    "    # Find best matches for each candidate\n",
    "    for candidate_idx, candidate_name in enumerate(candidates_df['name']):\n",
    "        # For each candidate, get the best matches (top experts)\n",
    "        best_experts_idx = np.argsort(-similarity_matrix[candidate_idx])  # Sort indices based on similarity scores in descending order\n",
    "        for expert_idx in best_experts_idx:\n",
    "            expert_name = experts_df.iloc[expert_idx]['name']\n",
    "            similarity_score = similarity_matrix[candidate_idx, expert_idx]\n",
    "            results.append((candidate_name, expert_name, similarity_score))\n",
    "\n",
    "    # Create a dictionary to store similarity scores\n",
    "    similarity_scores = {}\n",
    "    for candidate, expert, score in results:\n",
    "        if candidate not in similarity_scores:\n",
    "            similarity_scores[candidate] = {}\n",
    "        similarity_scores[candidate][expert] = score\n",
    "\n",
    "    return similarity_scores\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alice': {'Dr. Johnson': 0.8333333, 'Dr. Smith': 0.8181818, 'Dr. Brown': 0.476406, 'Dr. Taylor': 0.32970056}, 'Bob': {'Dr. Taylor': 0.37846094, 'Dr. Johnson': 0.22294234, 'Dr. Smith': 0.21565665, 'Dr. Brown': 0.19421607}, 'Charlie': {'Dr. Johnson': 0.6545541, 'Dr. Smith': 0.6451496, 'Dr. Brown': 0.41207463, 'Dr. Taylor': 0.31671673}, 'David': {'Dr. Taylor': 0.82087, 'Dr. Johnson': 0.3387207, 'Dr. Smith': 0.33065593, 'Dr. Brown': 0.24446523}}\n"
     ]
    }
   ],
   "source": [
    "print(Bipartite_graph_matching(candidates_data, experts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity Scores for All Individuals:\n",
      "Individual [0, 1, 0]: {'Alice': {'Dr. Smith': 0.7073623}, 'Bob': {'Dr. Johnson': 0.8378098}, 'Charlie': {'Dr. Smith': 0.5688001}}\n",
      "Individual [0, 0, 0]: {'Alice': {'Dr. Smith': 0.7073623}, 'Bob': {'Dr. Smith': 0.086992055}, 'Charlie': {'Dr. Smith': 0.5688001}}\n",
      "{'[0, 1, 0]': {'Alice': {'Dr. Smith': 0.7073623}, 'Bob': {'Dr. Johnson': 0.8378098}, 'Charlie': {'Dr. Smith': 0.5688001}}, '[0, 0, 0]': {'Alice': {'Dr. Smith': 0.7073623}, 'Bob': {'Dr. Smith': 0.086992055}, 'Charlie': {'Dr. Smith': 0.5688001}}}\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# print(Multi_Objective_Genetic_Algorithms(candidates_data, experts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alice': {'Dr. Smith': 1.0}, 'Charlie': {'Dr. Smith': 1.0}, 'Bob': {'Dr. Johnson': 1.0}}\n"
     ]
    }
   ],
   "source": [
    "# print(K_means_Clustering_Algorithm(candidates_data, experts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_recommendation_algorithm(candidates_data, experts_data):    \n",
    "    # Create DataFrames\n",
    "    candidates_df = pd.DataFrame(candidates_data)\n",
    "    experts_df = pd.DataFrame(experts_data)\n",
    "\n",
    "    # Combine skills to create a vocabulary\n",
    "    all_skills = candidates_df['skills'].tolist() + experts_df['skills'].tolist()\n",
    "\n",
    "    # Vectorize skills\n",
    "    vectorizer = CountVectorizer()\n",
    "    skills_matrix = vectorizer.fit_transform(all_skills)\n",
    "\n",
    "    # Split into candidates and experts features\n",
    "    candidates_skills_matrix = skills_matrix[:len(candidates_df)]\n",
    "    experts_skills_matrix = skills_matrix[len(candidates_df):]\n",
    "\n",
    "    # Convert to DataFrames\n",
    "    candidates_skills_df = pd.DataFrame(candidates_skills_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    experts_skills_df = pd.DataFrame(experts_skills_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "    # Add experience as a feature\n",
    "    candidates_features = pd.concat([candidates_skills_df, candidates_df['experience']], axis=1)\n",
    "    experts_features = pd.concat([experts_skills_df, experts_df['experience']], axis=1)\n",
    "\n",
    "    # Convert all column names to strings\n",
    "    candidates_features.columns = candidates_features.columns.astype(str)\n",
    "    experts_features.columns = experts_features.columns.astype(str)\n",
    "\n",
    "    # Ensure all features are numeric\n",
    "    candidates_features = candidates_features.apply(pd.to_numeric, errors='ignore')\n",
    "    experts_features = experts_features.apply(pd.to_numeric, errors='ignore')\n",
    "\n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    candidates_features_scaled = scaler.fit_transform(candidates_features)\n",
    "    experts_features_scaled = scaler.transform(experts_features)\n",
    "\n",
    "    # Combine all features for collaborative filtering\n",
    "    all_features_scaled = np.vstack([candidates_features_scaled, experts_features_scaled])\n",
    "\n",
    "    # Perform Dimensionality Reduction\n",
    "    svd = TruncatedSVD(n_components=10)\n",
    "    all_features_reduced = svd.fit_transform(all_features_scaled)\n",
    "\n",
    "    # Calculate cosine similarity for collaborative filtering\n",
    "    similarity_matrix = cosine_similarity(all_features_reduced)\n",
    "\n",
    "    # Split back into candidates and experts similarity matrix\n",
    "    candidates_similarity_matrix = similarity_matrix[:len(candidates_df), len(candidates_df):]\n",
    "    experts_similarity_matrix = similarity_matrix[len(candidates_df):, :len(candidates_df)]\n",
    "\n",
    "    # Transpose the experts_similarity_matrix to match the shape\n",
    "    experts_similarity_matrix = experts_similarity_matrix.T\n",
    "\n",
    "    # Define weights for hybrid recommendation\n",
    "    alpha = 0.5  # Weight for content-based scores\n",
    "    beta = 0.5   # Weight for collaborative scores\n",
    "\n",
    "    # Compute final scores\n",
    "    final_scores = alpha * candidates_similarity_matrix + beta * experts_similarity_matrix\n",
    "    experts_df['normalized_experience'] = experts_df['experience'] / experts_df['experience'].max()\n",
    "\n",
    "    for i in range(len(candidates_df)):\n",
    "        for j in range(len(experts_df)):\n",
    "            # Calculate the absolute difference in experience between candidate and expert\n",
    "            experience_diff = abs(candidates_df['experience'].iloc[i] - experts_df['experience'].iloc[j])\n",
    "            \n",
    "            # Compute inverse experience ratio (add a small epsilon to avoid division by zero)\n",
    "            epsilon = 1e-6\n",
    "            experience_ratio = 1 / (experience_diff + epsilon)\n",
    "            \n",
    "            # Normalize the experience ratio (optional for scaling)\n",
    "            experience_ratio /= (1 + experience_ratio)\n",
    "            \n",
    "            # Adjust the final score based on the experience ratio\n",
    "            final_scores[i, j] *= (1 - experience_weight + experience_weight * experience_ratio)\n",
    "\n",
    "    # Construct output in desired format\n",
    "    output_similarity_matrix = {}\n",
    "    for i, candidate_name in enumerate(candidates_df['name']):\n",
    "        output_similarity_matrix[candidate_name] = {}\n",
    "        for j, expert_name in enumerate(experts_df['name']):\n",
    "            output_similarity_matrix[candidate_name][expert_name] = final_scores[i, j]\n",
    "\n",
    "    return output_similarity_matrix\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Alice': {'Dr. Johnson': 0.672152479572753, 'Dr. Smith': 0.4445580676203108, 'Dr. Brown': 0.06636245976450182, 'Dr. Taylor': -0.30156318455931214}, 'Bob': {'Dr. Johnson': -0.1919659288904818, 'Dr. Smith': -0.06704293235792054, 'Dr. Brown': 0.04121712974441467, 'Dr. Taylor': -0.17659959187130372}, 'Charlie': {'Dr. Johnson': -0.1852625648798173, 'Dr. Smith': -0.20074282779498814, 'Dr. Brown': -0.07343747072075271, 'Dr. Taylor': -0.32181506778530855}, 'David': {'Dr. Johnson': -0.24958310976542486, 'Dr. Smith': -0.1511172216734905, 'Dr. Brown': -0.034049975615631764, 'Dr. Taylor': 0.7574836205993246}}\n"
     ]
    }
   ],
   "source": [
    "print(hybrid_recommendation_algorithm(candidates_data, experts_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_similarity_with_BERT_embeddings_matrix = cosine_similarity_with_BERT_embeddings(candidates_data, experts_data)\n",
    "cosine_similarity_with_topic_modelling_matrix = cosine_similarity_with_topic_modelling(candidates_data, experts_data)\n",
    "Faiss_search_matrix = Faiss_search(candidates_data, experts_data)\n",
    "Bipartite_graph_matching_matrix = Bipartite_graph_matching(candidates_data, experts_data)\n",
    "hybrid_recommendation_algorithm_matrix = hybrid_recommendation_algorithm(candidates_data, experts_data)\n",
    "\n",
    "# without precomputation - 8.9 s\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Unique Matching based on Combined Scores:\n",
      "Candidate David is best matched with Expert Dr. Taylor (Combined Score: 0.98)\n",
      "Candidate Alice is best matched with Expert Dr. Johnson (Combined Score: 0.90)\n",
      "Candidate Bob is best matched with Expert Dr. Smith (Combined Score: 0.76)\n",
      "Candidate Charlie is best matched with Expert Dr. Johnson (Combined Score: 0.30)\n"
     ]
    }
   ],
   "source": [
    "# Example scores from each method with desired format\n",
    "methods_scores = {\n",
    "    'cosine_similarity_with_topic_modelling': cosine_similarity_with_topic_modelling_matrix,\n",
    "    'cosine_similarity_with_BERT_embeddings': cosine_similarity_with_BERT_embeddings_matrix,\n",
    "    'faiss_searching': Faiss_search_matrix,\n",
    "    'hybrid_recommendation': hybrid_recommendation_algorithm_matrix\n",
    "}\n",
    "\n",
    "# Normalizing function\n",
    "def normalize_scores(scores):\n",
    "    all_scores = [v for candidate_scores in scores.values() for v in candidate_scores.values()]\n",
    "    min_score = min(all_scores)\n",
    "    max_score = max(all_scores)\n",
    "    return {candidate: {expert: (score - min_score) / (max_score - min_score)\n",
    "                        for expert, score in candidate_scores.items()}\n",
    "            for candidate, candidate_scores in scores.items()}\n",
    "\n",
    "# Normalize all methods' scores\n",
    "normalized_methods_scores = {method: normalize_scores(scores) for method, scores in methods_scores.items()}\n",
    "\n",
    "# Step 2: Combine the scores\n",
    "# Assigning equal weight to all methods\n",
    "weights = {method: 1 / len(normalized_methods_scores) for method in normalized_methods_scores}\n",
    "\n",
    "combined_scores = {candidate: {} for candidate in normalized_methods_scores['faiss_searching'].keys()}\n",
    "\n",
    "for candidate in combined_scores.keys():\n",
    "    for expert in normalized_methods_scores['faiss_searching'][candidate].keys():\n",
    "        combined_score = sum(weights[method] * normalized_methods_scores[method][candidate][expert]\n",
    "                             for method in normalized_methods_scores)\n",
    "        combined_scores[candidate][expert] = combined_score\n",
    "\n",
    "# Step 3: Rank the matches and ensure unique matches per candidate\n",
    "final_matches = sorted([(candidate, expert, score) for candidate, experts in combined_scores.items() \n",
    "                        for expert, score in experts.items()],\n",
    "                       key=lambda x: x[2], reverse=True)\n",
    "\n",
    "# Ensure unique matching per candidate\n",
    "assigned_candidates = set()\n",
    "unique_matches = []\n",
    "\n",
    "# Iterate over sorted matches\n",
    "for match in final_matches:\n",
    "    candidate, expert, score = match\n",
    "    if candidate not in assigned_candidates:\n",
    "        unique_matches.append(match)\n",
    "        assigned_candidates.add(candidate)\n",
    "\n",
    "# Display final unique matches\n",
    "print(\"Final Unique Matching based on Combined Scores:\")\n",
    "for match in unique_matches:\n",
    "    print(f\"Candidate {match[0]} is best matched with Expert {match[1]} (Combined Score: {match[2]:.2f})\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # takes too much time\n",
    "# def Multi_Objective_Genetic_Algorithms(candidates_data, experts_data):\n",
    "#     # Convert data to DataFrames\n",
    "#     candidates_df = pd.DataFrame(candidates_data)\n",
    "#     experts_df = pd.DataFrame(experts_data)\n",
    "\n",
    "#     # Load pre-trained Sentence-BERT model to generate embeddings\n",
    "#     model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "#     # Generate embeddings for candidate and expert skills\n",
    "#     candidates_embeddings = model.encode(candidates_df['skills'].tolist())\n",
    "#     experts_embeddings = model.encode(experts_df['skills'].tolist())\n",
    "\n",
    "#     # Compute cosine similarity between each candidate and expert\n",
    "#     similarity_matrix = cosine_similarity(candidates_embeddings, experts_embeddings)\n",
    "\n",
    "#     # Create a similarity matrix DataFrame\n",
    "#     similarity_matrix_df = pd.DataFrame(similarity_matrix,\n",
    "#                                         index=candidates_df['name'],\n",
    "#                                         columns=experts_df['name'])\n",
    "\n",
    "#     # Genetic Algorithm Setup\n",
    "#     creator.create(\"FitnessMulti\", base.Fitness, weights=(1.0, -1.0))  # Maximize similarity, minimize experience difference\n",
    "#     creator.create(\"Individual\", list, fitness=creator.FitnessMulti)\n",
    "\n",
    "#     toolbox = base.Toolbox()\n",
    "\n",
    "#     # Initialize individual with random assignment of experts to candidates, allowing duplicates\n",
    "#     def init_individual():\n",
    "#         return random.choices(range(len(experts_df)), k=len(candidates_df))\n",
    "\n",
    "#     toolbox.register(\"individual\", tools.initIterate, creator.Individual, init_individual)\n",
    "#     toolbox.register(\"population\", tools.initRepeat, list, toolbox.individual)\n",
    "\n",
    "#     def evaluate(individual):\n",
    "#         total_similarity = 0.0\n",
    "#         total_experience_difference = 0.0\n",
    "        \n",
    "#         for i, expert_idx in enumerate(individual):\n",
    "#             candidate_experience = candidates_df.iloc[i]['experience']\n",
    "#             expert_experience = experts_df.iloc[expert_idx]['experience']\n",
    "            \n",
    "#             similarity = similarity_matrix[i, expert_idx]\n",
    "#             experience_difference = abs(candidate_experience - expert_experience)\n",
    "            \n",
    "#             total_similarity += similarity\n",
    "#             total_experience_difference += experience_difference\n",
    "        \n",
    "#         return total_similarity / len(individual), total_experience_difference / len(individual)\n",
    "\n",
    "#     toolbox.register(\"mate\", tools.cxTwoPoint)\n",
    "#     toolbox.register(\"mutate\", tools.mutShuffleIndexes, indpb=0.2)\n",
    "#     toolbox.register(\"select\", tools.selNSGA2)\n",
    "#     toolbox.register(\"evaluate\", evaluate)\n",
    "\n",
    "#     # Genetic Algorithm Execution\n",
    "#     population = toolbox.population(n=50)  # Population size\n",
    "#     ngen = 50  # Number of generations\n",
    "#     cxpb = 0.7  # Crossover probability\n",
    "#     mutpb = 0.2  # Mutation probability\n",
    "\n",
    "#     result_population = algorithms.eaMuPlusLambda(population, toolbox, mu=50, lambda_=100, cxpb=cxpb, mutpb=mutpb, ngen=ngen, \n",
    "#                                                 stats=None, halloffame=None, verbose=False)\n",
    "\n",
    "#     # Extract best individuals\n",
    "#     all_individuals = result_population[0]\n",
    "    \n",
    "#     # Create a dictionary to store the similarity scores for all individuals\n",
    "#     similarity_scores_all = {}\n",
    "\n",
    "#     for ind in all_individuals:\n",
    "#         individual_score = {}\n",
    "#         for i, expert_idx in enumerate(ind):\n",
    "#             candidate_name = candidates_df.iloc[i]['name']\n",
    "#             expert_name = experts_df.iloc[expert_idx]['name']\n",
    "#             similarity_score = similarity_matrix[i, expert_idx]\n",
    "\n",
    "#             if candidate_name not in individual_score:\n",
    "#                 individual_score[candidate_name] = {}\n",
    "            \n",
    "#             individual_score[candidate_name][expert_name] = similarity_score\n",
    "        \n",
    "#         similarity_scores_all[str(ind)] = individual_score\n",
    "\n",
    "#     # Print all matches with similarity scores\n",
    "#     print(\"Similarity Scores for All Individuals:\")\n",
    "#     for individual, scores in similarity_scores_all.items():\n",
    "#         print(f\"Individual {individual}: {scores}\")\n",
    "\n",
    "#     return similarity_scores_all\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def K_means_Clustering_Algorithm(candidates_data, experts_data):\n",
    "#     # Create DataFrames\n",
    "#     candidates_df = pd.DataFrame(candidates_data)\n",
    "#     experts_df = pd.DataFrame(experts_data)\n",
    "\n",
    "#     # Combine skills to create a vocabulary\n",
    "#     all_skills = candidates_df['skills'].tolist() + experts_df['skills'].tolist()\n",
    "\n",
    "#     # Vectorize skills\n",
    "#     vectorizer = CountVectorizer()\n",
    "#     skills_matrix = vectorizer.fit_transform(all_skills)\n",
    "\n",
    "#     # Split into candidates and experts features\n",
    "#     candidates_skills_matrix = skills_matrix[:len(candidates_df)]\n",
    "#     experts_skills_matrix = skills_matrix[len(candidates_df):]\n",
    "\n",
    "#     # Convert to DataFrames\n",
    "#     candidates_skills_df = pd.DataFrame(candidates_skills_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "#     experts_skills_df = pd.DataFrame(experts_skills_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "\n",
    "#     # Add experience as a feature\n",
    "#     candidates_features = pd.concat([candidates_skills_df, candidates_df['experience']], axis=1)\n",
    "#     experts_features = pd.concat([experts_skills_df, experts_df['experience']], axis=1)\n",
    "\n",
    "#     # Ensure 'experience' is numeric\n",
    "#     candidates_features['experience'] = pd.to_numeric(candidates_features['experience'], errors='coerce')\n",
    "#     experts_features['experience'] = pd.to_numeric(experts_features['experience'], errors='coerce')\n",
    "\n",
    "#     # Combine all features for standardization\n",
    "#     all_features = pd.concat([candidates_features, experts_features], axis=0)\n",
    "\n",
    "#     # Handle missing values (e.g., fill with 0)\n",
    "#     all_features.fillna(0, inplace=True)\n",
    "\n",
    "#     # Standardize the combined features\n",
    "#     scaler = StandardScaler()\n",
    "#     all_features_scaled = scaler.fit_transform(all_features)\n",
    "\n",
    "#     # Split back into candidates and experts\n",
    "#     candidates_features_scaled = all_features_scaled[:len(candidates_df)]\n",
    "#     experts_features_scaled = all_features_scaled[len(candidates_df):]\n",
    "\n",
    "#     # Apply K-Means\n",
    "#     kmeans = KMeans(n_clusters=len(experts_df), random_state=42)\n",
    "#     kmeans.fit(all_features_scaled)\n",
    "\n",
    "#     # Get cluster labels\n",
    "#     candidates_clusters = kmeans.predict(candidates_features_scaled)\n",
    "#     experts_clusters = kmeans.predict(experts_features_scaled)\n",
    "\n",
    "#     # Assign cluster labels back to DataFrames\n",
    "#     candidates_df['cluster'] = candidates_clusters\n",
    "#     experts_df['cluster'] = experts_clusters\n",
    "\n",
    "#     # Compute similarity matrix for each cluster\n",
    "#     combined_similarity_matrix = {}\n",
    "#     for cluster in np.unique(candidates_clusters):\n",
    "#         cluster_candidates = candidates_df[candidates_df['cluster'] == cluster]\n",
    "#         cluster_experts = experts_df[experts_df['cluster'] == cluster]\n",
    "        \n",
    "#         if not cluster_experts.empty and not cluster_candidates.empty:\n",
    "#             # Combine skills and experience for similarity calculation\n",
    "#             cluster_candidates_features = cluster_candidates.drop(columns=['id', 'name', 'cluster'])\n",
    "#             cluster_experts_features = cluster_experts.drop(columns=['id', 'name', 'cluster'])\n",
    "\n",
    "#             # Convert all columns to numeric and handle missing values\n",
    "#             cluster_candidates_features = cluster_candidates_features.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "#             cluster_experts_features = cluster_experts_features.apply(pd.to_numeric, errors='coerce').fillna(0)\n",
    "\n",
    "#             # Compute cosine similarity between candidates and experts in the same cluster\n",
    "#             similarity_matrix = cosine_similarity(cluster_candidates_features, cluster_experts_features)\n",
    "            \n",
    "#             # Store similarity scores in the desired dictionary format\n",
    "#             for i, candidate_name in enumerate(cluster_candidates['name']):\n",
    "#                 if candidate_name not in combined_similarity_matrix:\n",
    "#                     combined_similarity_matrix[candidate_name] = {}\n",
    "#                 for j, expert_name in enumerate(cluster_experts['name']):\n",
    "#                     combined_similarity_matrix[candidate_name][expert_name] = similarity_matrix[i][j]\n",
    "\n",
    "#     return combined_similarity_matrix\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
